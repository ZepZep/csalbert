{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load SQAD predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sqad(dataset, model):\n",
    "    with open(f\"dataset/{dataset}_dev.json\") as f:\n",
    "        dev = json.load(f)\n",
    "\n",
    "    counts = []\n",
    "    questions = {}\n",
    "\n",
    "    for doc in dev[\"data\"]:\n",
    "        for par in doc[\"paragraphs\"]:\n",
    "            counts.append(len(par[\"qas\"]))\n",
    "            for qq in par[\"qas\"]:\n",
    "                if not qq[\"answers\"]:\n",
    "                    continue\n",
    "                q = {\n",
    "                    \"question\": qq[\"question\"],\n",
    "                    \"answer\": qq[\"answers\"][0][\"text\"],\n",
    "                    \"context\": par[\"context\"]\n",
    "                }\n",
    "                questions[qq[\"id\"]] = q\n",
    "\n",
    "#     pprint(next(iter(questions.items())))\n",
    "\n",
    "    with open(f\"models/{model}_{dataset}/nbest_predictions.json\") as f:\n",
    "        pred = json.load(f)\n",
    "        \n",
    "    return questions, pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Longest common sequence getter\n",
    "find the overlap between answer and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lcs(X, Y, split=True):\n",
    "    if split:\n",
    "        X = X.split(\" \")\n",
    "        Y = Y.split(\" \")\n",
    "    m = len(X)\n",
    "    n = len(Y)\n",
    "    \n",
    "    L = [[0 for x in range(n+1)] for x in range(m+1)] \n",
    "  \n",
    "    # Following steps build L[m+1][n+1] in bottom up fashion. Note \n",
    "    # that L[i][j] contains length of LCS of X[0..i-1] and Y[0..j-1]  \n",
    "    for i in range(m+1): \n",
    "        for j in range(n+1): \n",
    "            if i == 0 or j == 0: \n",
    "                L[i][j] = 0\n",
    "            elif X[i-1] == Y[j-1]: \n",
    "                L[i][j] = L[i-1][j-1] + 1\n",
    "            else: \n",
    "                L[i][j] = max(L[i-1][j], L[i][j-1]) \n",
    "  \n",
    "    # Following code is used to print LCS \n",
    "    index = L[m][n] \n",
    "  \n",
    "    # Create a character array to store the lcs string \n",
    "    lcs = [\"\"] * (index+1) \n",
    "    lcs[index] = \"\" \n",
    "  \n",
    "    # Start from the right-most-bottom-most corner and \n",
    "    # one by one store characters in lcs[] \n",
    "    i = m \n",
    "    j = n \n",
    "    while i > 0 and j > 0: \n",
    "  \n",
    "        # If current character in X[] and Y are same, then \n",
    "        # current character is part of LCS \n",
    "        if X[i-1] == Y[j-1]: \n",
    "            lcs[index-1] = X[i-1] \n",
    "            i-=1\n",
    "            j-=1\n",
    "            index-=1\n",
    "  \n",
    "        # If not same, then find the larger of two and \n",
    "        # go in the direction of larger value \n",
    "        elif L[i-1][j] > L[i][j-1]: \n",
    "            i-=1\n",
    "        else: \n",
    "            j-=1\n",
    "  \n",
    "    if split:\n",
    "        return \" \".join(lcs[:-1])\n",
    "    return \"\".join(lcs[:-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive question/answer viewer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "def highlight_answer(text, answer):\n",
    "    sub = lcs(text, answer, False)\n",
    "    if sub:\n",
    "        l = text.split(sub)\n",
    "        return f'<span style=\"background-color: #CCCC00\">{sub}</span>'.join(l).replace(\"\\n\", \"<br>\")\n",
    "    return text\n",
    "\n",
    "def prediction_viewer(questions, pred):\n",
    "    keys = list(questions.keys())\n",
    "    slider = widgets.IntSlider(\n",
    "        value=0,\n",
    "        min=0,\n",
    "        max=len(keys),\n",
    "        step=1,\n",
    "        description='Question:',\n",
    "        disabled=False,\n",
    "        continuous_update=False,\n",
    "        orientation='horizontal',\n",
    "        readout=True,\n",
    "        readout_format='d'\n",
    "    )\n",
    "\n",
    "    left = widgets.Output(layout={\"width\": \"50%\"})\n",
    "    right = widgets.Output(layout={\"width\": \"50%\"})\n",
    "\n",
    "\n",
    "    def f(val):\n",
    "        key = keys[val]\n",
    "        q = questions[key]\n",
    "        \n",
    "        left.clear_output()\n",
    "        with left:\n",
    "            display(HTML(\n",
    "                key + \"<hr>\" + q[\"question\"] + \"<hr>\" + q[\"answer\"] + \"<hr>\" + highlight_answer(q[\"context\"], q[\"answer\"])\n",
    "            ))\n",
    "        \n",
    "        right.clear_output()\n",
    "        with right:\n",
    "            display(HTML(\n",
    "                \"<hr>\".join(\n",
    "                    highlight_answer(a[\"text\"], q[\"answer\"]).replace(\"\\n\", \"<br>\") + \"<br>\" + \"\"\n",
    "                    f\"prob: {a['probability']:5.2f} | slp: {a['start_log_prob']:5.2f} | elp: {a['end_log_prob']:5.2f}\"\n",
    "                        for a in pred[key]\n",
    "                )\n",
    "            ))\n",
    "        \n",
    "        \n",
    "    out = widgets.interactive_output(f, {'val': slider})\n",
    "    display(widgets.VBox([\n",
    "        slider,\n",
    "        widgets.HBox([left, right])\n",
    "    ]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_match_num(row):\n",
    "    try:\n",
    "        i = row.guesses.index(row.answer)\n",
    "        return i\n",
    "    except ValueError:\n",
    "        try:\n",
    "            i = row.guesses.index(row.answer+\",\")\n",
    "            return i\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "        return 11\n",
    "\n",
    "    \n",
    "def coverage(answer, prediction):\n",
    "    common = lcs(answer, prediction, split=False)\n",
    "    return len(common) / max(len(answer), len(prediction))\n",
    "\n",
    "def coverage_list(answer, predictions):\n",
    "    return max(coverage(answer, pred) for pred in predictions)\n",
    "\n",
    "def csformat(num):\n",
    "    return f\"{num:.2f}\".replace(\".\", \",\")\n",
    "    \n",
    "def metrics(adf, count):\n",
    "    vc = adf.answer_index.value_counts()\n",
    "    match = vc.loc[range(count)].sum() / len(adf)\n",
    "    \n",
    "    fcn = lambda row: coverage_list(row.answer, row.guesses[:count])\n",
    "    cov = adf.apply(fcn, axis=1).mean()\n",
    "    \n",
    "    return csformat(match), csformat(cov)\n",
    "\n",
    "def evaluate_results(questions, pred):\n",
    "    for i, key in enumerate(questions.keys()):\n",
    "        guesses = pred[key]\n",
    "        questions[key][\"guesses\"] = [g[\"text\"] for g in guesses]\n",
    "        questions[key][\"index\"] = i\n",
    "\n",
    "    adf = pd.DataFrame.from_dict(questions, orient=\"index\")\n",
    "    adf[\"answer_index\"] = adf.apply(get_match_num, axis=1)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"exact\": metrics(adf, 1),\n",
    "        \"top5\": metrics(adf, 5),\n",
    "        \"top10\": metrics(adf, 9),\n",
    "    }, index=[f\"match\", f\"coverage\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exact</th>\n",
       "      <th>top5</th>\n",
       "      <th>top10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>match</th>\n",
       "      <td>0,18</td>\n",
       "      <td>0,37</td>\n",
       "      <td>0,42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coverage</th>\n",
       "      <td>0,34</td>\n",
       "      <td>0,64</td>\n",
       "      <td>0,70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         exact  top5 top10\n",
       "match     0,18  0,37  0,42\n",
       "coverage  0,34  0,64  0,70"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions, predictions = load_sqad(\"sqad_extract\", \"csbase3\")\n",
    "evaluate_results(questions, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View questions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99484208076e43a59f5683638de6ebca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntSlider(value=0, continuous_update=False, description='Question:', max=558), HBox(children=(O…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction_viewer(questions, predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
