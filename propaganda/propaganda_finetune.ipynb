{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from itertools import cycle\n",
    "import math\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from collections import defaultdict, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "InOut = namedtuple('InOut', ['x', 'y'])\n",
    "TrainDev = namedtuple('TrainDev', ['train', 'dev'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_key = {                                                      \n",
    "    'zanr':  ['zpravodajství', 'rozhovor', 'komentář'],                       \n",
    "    'tema': ['migrační krize', 'domácí politika',                             \n",
    "        'zahraniční politika / diplomacie',                                   \n",
    "        'společnost / společenská situace', 'jiné', 'energetika',             \n",
    "        'sociální politika', 'konflikt na Ukrajině', 'kultura',               \n",
    "        'konflikt v Sýrii', 'zbrojní politika', 'ekonomika / finance',        \n",
    "        'konspirace'],                                                        \n",
    "    'zamereni': ['zahraniční', 'domácí', 'obojí', 'nelze určit'],             \n",
    "    'lokace': ['EU', 'Česká republika', 'USA', 'jiná země',                   \n",
    "        'jiné / nelze určit', 'Rusko', 'NATO', 'Rusko + USA'],                \n",
    "    'argumentace': ['ne', 'ano'],                                             \n",
    "    'emoce': ['missing', 'rozhořčení', 'soucit', 'strach', 'nenávist', 'jiná'],\n",
    "    'vyzneni_celku': ['neutrální', 'negativní', 'pozitivní'],                 \n",
    "    'rusko': ['missing', 'pozitivní příklad', 'neutrální', 'oběť',            \n",
    "        'negativní příklad', 'hrdina'],                                       \n",
    "    'vyzneni1': \n",
    "        ['neutrální', 'negativní', 'missing', 'pozitivní', 'velebící', 'nenávistné'],                                                        \n",
    "    'vyzneni2':\n",
    "        ['neutrální', 'negativní', 'missing', 'pozitivní', 'velebící', 'nenávistné'],                                                        \n",
    "    'vyzneni3':\n",
    "        ['neutrální', 'negativní', 'missing', 'pozitivní', 'velebící', 'nenávistné'],                                                        \n",
    "    'obrazek': ['ne', 'ano'],                                                 \n",
    "    'video': ['ne', 'ano'],                                                   \n",
    "    'nazor': ['ne', 'ano'],                                                   \n",
    "    'odbornik': ['ne', 'ano'],                                                \n",
    "    'zdroj': ['ne', 'ano'],                                                   \n",
    "    'strach': ['ne', 'ano'],                                                  \n",
    "    'vina': ['ne', 'ano'],                                                    \n",
    "    'nalepkovani': ['ne', 'ano'],                                             \n",
    "    'demonizace': ['ne', 'ano'],                                              \n",
    "    'relativizace': ['ne', 'ano'],                                                                                                                           \n",
    "    'fabulace': ['ne', 'ano'],                                                \n",
    "    'year': ['2016', '2017', '2018']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(path):\n",
    "    df = pd.read_csv(\"dataset/data_raw.csv\")\n",
    "    df.tema[df.tema == \"sociálni politika\"] = \"sociální politika\"\n",
    "    df.head()\n",
    "    \n",
    "    x_column = \"text\"\n",
    "    \n",
    "    x = df[x_column]\n",
    "    \n",
    "    ydf = pd.DataFrame()\n",
    "    for column in df.loc[:, df.columns != x_column]:\n",
    "        ydf[column] = df[column].map(lambda x: feature_key[column].index(x))\n",
    "    y = {col: ydf[col].to_numpy() for col in ydf}\n",
    "    \n",
    "    return InOut(x, y)\n",
    "\n",
    "dataset = get_dataset(\"dataset/data_raw.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Czech ALBERT model definitions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_defs = {\n",
    "    # example: (path, width, batch_size)\n",
    "    \"csbase3\": (\"pretrained/csbase3_ckpt/\", 256, 24),\n",
    "    \"csbase4\": (\"pretrained/csbase4_ckpt/\", 256, 16),\n",
    "    \"cslarge3\": (\"pretrained/cslarge3_ckpt/\", 512, 12)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "class SPMEmbedder():\n",
    "    def __init__(self, path):\n",
    "        sp = spm.SentencePieceProcessor()\n",
    "        sp.Load(path)\n",
    "        self.sp = sp\n",
    "        \n",
    "    def encode(self, text):\n",
    "        return [self.sp.piece_to_id(\"[CLS]\"),\n",
    "                *self.sp.EncodeAsIds(text), \n",
    "                self.sp.piece_to_id(\"[SEP]\")\n",
    "               ]\n",
    "    \n",
    "    def decode(self, indexes):\n",
    "        return self.sp.DecodeIds(list(map(int, indexes)))\n",
    "    \n",
    "    def decode_pieces(self, indexes):\n",
    "        return \" \".join(self.sp.id_to_piece(int(id_)) for id_ in indexes)\n",
    "    \n",
    "    def get_embed_fcn(self, width):\n",
    "        def embed(sentence):\n",
    "            return pad_sequences([self.encode(sentence)], width,\n",
    "                                 padding=\"post\", truncating=\"post\")[0]\n",
    "        return embed \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"[0-9]\", \"#\", text)\n",
    "    text = re.sub(\"[‒–—―]\", \"-\", text)\n",
    "    text = re.sub(\"[“”‘’„“‚‘\\\"']\", \"'\", text)\n",
    "    text = re.sub(\"[^0-9a-zóěščřžýáíďéťňůúA-ZÓĚŠČŘŽÝÁÍĎÉŤŇŮÚ\\\\.,\\\\!\\\\?%\\\\(\\\\)\\\\-'#: ]\", \"\", text)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "def get_training_data(dataset, width, task, verbose=False):\n",
    "    x, y = dataset.x, dataset.y[task]\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=RANDOM_STATE)\n",
    "    \n",
    "    train_index, test_index = next(sss.split(x, y))\n",
    "    train = InOut(x[train_index], y[train_index])\n",
    "    dev = InOut(x[test_index], y[test_index])\n",
    "\n",
    "    return TrainDev(train, dev)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALBERT model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras as keras\n",
    "import bert\n",
    "\n",
    "from pprint import pprint\n",
    "from IPython.utils import io\n",
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def get_albert_layer(path, max_seq_len, name, trainable=True):\n",
    "    albert_params = bert.albert_params(path)\n",
    "    l_albert = bert.BertModelLayer.from_params(albert_params, name=name, \n",
    "                                             shared_layer=True, trainable=trainable)\n",
    "    return l_albert\n",
    "    \n",
    "def get_auto_model(path, max_seq_len, feature_key, task, name, albert_trainable=True, init=True):\n",
    "    l_input_ids = keras.layers.Input(shape=(max_seq_len,), dtype='int32', name=\"input_tokens\")\n",
    "    l_albert = get_albert_layer(path, max_seq_len, \"albert\", albert_trainable)\n",
    "    l_middle = l_albert(l_input_ids)\n",
    "    cls_embed = keras.layers.Lambda(lambda seq: seq[:, 0, :], name=\"cls_selector\")(l_middle)\n",
    "    \n",
    "    num_classes = len(feature_key[task])\n",
    "    taskname = task\n",
    "    l_output = keras.layers.Dense(num_classes, activation='softmax', name=taskname)(cls_embed)\n",
    "\n",
    "    \n",
    "    model = keras.Model(inputs=l_input_ids, outputs=l_output, name=name)\n",
    "    opt = keras.optimizers.Adam(learning_rate=1e-5, beta_1=0.9, beta_2=0.999, amsgrad=True)\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss=\"sparse_categorical_crossentropy\",\n",
    "                  metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",
    "    )\n",
    "\n",
    "    if init:\n",
    "        with io.capture_output() as captured:\n",
    "            bert.load_albert_weights(l_albert, path+\"model.ckpt-best\")\n",
    "            notfound = re.search(r\"Count of weights not found in the checkpoint was: \\[([0-9]*)\\].\",\n",
    "                                 captured.stdout\n",
    "                                ).groups(1)[0]\n",
    "\n",
    "        print(\"Weights loaded.\", notfound, \"not found.\\n\")\n",
    "    else:\n",
    "        print(\"Skipping weight loading.\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callback for evaluation during training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, balanced_accuracy_score\n",
    "\n",
    "class BatchCB(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, dev_set):\n",
    "        self.logs = []\n",
    "        self.devset = dev_set\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs=None):        \n",
    "        predictions = self.model.predict(self.devset.x)\n",
    "        pred_labels = np.argmax(predictions, axis=1)\n",
    "        acc = accuracy_score(self.devset.y, pred_labels)\n",
    "        bal_acc = balanced_accuracy_score(self.devset.y, pred_labels)\n",
    "        wf1 = f1_score(self.devset.y, pred_labels, average=\"weighted\")\n",
    "\n",
    "        print(f\"{' '*10} || {acc=:.3f} | {bal_acc=:.3f} | {wf1=:.3f} \", end=\"\")\n",
    "        \n",
    "        logs = dict(logs)\n",
    "        logs[\"val_acc\"] = acc\n",
    "        logs[\"val_bal_acc\"] = bal_acc\n",
    "        logs[\"val_wf1\"] = wf1\n",
    "        \n",
    "        self.logs.append(logs)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_weights(y):\n",
    "    y = pd.Series(y)\n",
    "    return (len(y)/y.value_counts()/len(y.value_counts()))\n",
    "\n",
    "\n",
    "def eval_model(model_name, dataset, tasks, dataset_name=\"prop\", init=True, verbose=False):\n",
    "    model_path, max_seq_len, batch_size = model_defs[model_name]\n",
    "    \n",
    "    print(f\"Tokenizing for {model_name}, width: {max_seq_len} ...\", end=\"\", flush=True)\n",
    "    x = dataset.x\n",
    "    embedder = SPMEmbedder(f\"{model_path}/spm.model\")\n",
    "    x = np.stack(x.map(lemmatize).map(embedder.get_embed_fcn(max_seq_len)))\n",
    "    dataset = InOut(x, dataset.y)\n",
    "    print(\" Done\")\n",
    "    \n",
    "    for task in tasks:\n",
    "        \n",
    "        display(HTML(f\"<h1 style=\\\"color: blue\\\">{task}</h1>\"))\n",
    "        train, dev = get_training_data(dataset, max_seq_len, task, verbose=verbose)\n",
    "        class_coefs = get_class_weights(train.y)\n",
    "        \n",
    "        model = get_auto_model(model_path,\n",
    "                               max_seq_len,\n",
    "                               feature_key,\n",
    "                               task,\n",
    "                               f\"{dataset_name}_{model_name}_{task}\",\n",
    "                               albert_trainable=True,\n",
    "                               init=init\n",
    "        )\n",
    "        \n",
    "        print()\n",
    "        model.summary()\n",
    "        \n",
    "        last_epoch = 0\n",
    "        cur_coefs = {k: v for k, v in class_coefs.items()}\n",
    "        epochs_per_loop = 1\n",
    "        bcb = BatchCB(dev)\n",
    "        \n",
    "        for _ in range(6):\n",
    "            model.fit(\n",
    "                train.x,\n",
    "                train.y,\n",
    "                class_weight=cur_coefs,\n",
    "                batch_size=batch_size,\n",
    "                epochs=last_epoch+epochs_per_loop,\n",
    "                initial_epoch=last_epoch,\n",
    "                callbacks=[\n",
    "                    bcb,\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            last_epoch += epochs_per_loop\n",
    "            cur_coefs = {k: v**0.7 for k, v in cur_coefs.items()}\n",
    "            \n",
    "        results = pd.DataFrame({\n",
    "            \"train_acc\": [x[\"acc\"] for x in bcb.logs],\n",
    "            \"val_acc\": [x[\"val_acc\"] for x in bcb.logs],\n",
    "            \"val_bal_acc\": [x[\"val_bal_acc\"] for x in bcb.logs],\n",
    "            \"val_wf1\": [x[\"val_wf1\"] for x in bcb.logs],                                \n",
    "        })\n",
    "        \n",
    "        vf1 = results.val_wf1.max()\n",
    "        vacc = results.val_acc.max()\n",
    "        \n",
    "        plot = results.plot(figsize=(7,4), title=f\"{task} - {vacc:.3f} | {vf1:.3f}\")\n",
    "        plt.show()\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing for csbase3, width: 256 ... Done\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h1 style=\"color: blue\">argumentace</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights loaded. 0 not found.\n",
      "\n",
      "\n",
      "Model: \"prop_csbase3_argumentace\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_tokens (InputLayer)    [(None, 256)]             0         \n",
      "_________________________________________________________________\n",
      "albert (BertModelLayer)      (None, 256, 256)          5221120   \n",
      "_________________________________________________________________\n",
      "cls_selector (Lambda)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "argumentace (Dense)          (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 5,221,634\n",
      "Trainable params: 5,221,634\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " 51/289 [====>.........................] - ETA: 1:45 - loss: 0.6863 - acc: 0.6111"
     ]
    }
   ],
   "source": [
    "eval_model(\"csbase3\", dataset, [\n",
    "    \"argumentace\", \n",
    "    \"lokace\", \n",
    "    \"zdroj\", \n",
    "    \"rusko\", \n",
    "    \"odbornik\", \n",
    "    \"tema\", \n",
    "    \"zamereni\", \n",
    "], init=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
